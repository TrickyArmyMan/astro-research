{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e36d9055-bd86-40a8-a0fd-3da9e4b47fd1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Parallel Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f26f416-0af1-479e-a5c6-7efe1743784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot\n",
    "import pandas\n",
    "import glob\n",
    "import emcee\n",
    "\n",
    "import eztao\n",
    "import eztao.ts\n",
    "\n",
    "import celerite\n",
    "\n",
    "import pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71b45e37-239b-4113-871a-48ade212d7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Define CARMA function for DRW\n",
    "################################\n",
    "\n",
    "def get_carma_parameter(tau, amp):\n",
    "    \"\"\"Get DRW parameters in CARMA notation (alpha_*/beta_*).\n",
    "\n",
    "    alpha_1 = -1 / tau\n",
    "    sigma^2 = tau * sigma_kbs^2 / 2\n",
    "    sigma_kbs = np.sqrt( 2 * sigma^2 / tau )\n",
    "    beta_0 = sigma_kbs\n",
    "\n",
    "    Returns:\n",
    "        [alpha_1, beta_0].\n",
    "    \"\"\"\n",
    "    return [-1.0 / tau, numpy.sqrt( 2.0 * amp**2.0 / tau)]\n",
    "\n",
    "################################\n",
    "# Define the prior and log-probability functions for MCMC\n",
    "################################\n",
    "\n",
    "# prior function for tau_perturb\n",
    "def lnprior_perturb(theta):\n",
    "    \"\"\"Prior on perturbation timescale. Note: this is a wedge like prior.\"\"\"\n",
    "\n",
    "    # determine DHO timescales\n",
    "    log10_tau_perturb = (theta[-1] - theta[-2])/numpy.log(10)\n",
    "    if -3 <= log10_tau_perturb <= 5:\n",
    "        prior = 0\n",
    "    else:\n",
    "        prior = -(numpy.abs(log10_tau_perturb - 1) - 4)\n",
    "\n",
    "    return prior\n",
    "\n",
    "def lnprior_bounds(theta):\n",
    "    \"\"\"Prior on AR and MA parameters. This is a flat prior.\"\"\"\n",
    "\n",
    "    # Place some bounds on the parameter space\n",
    "    bounds_low = numpy.array([-15, -15, -20, -20])\n",
    "    bounds_high = numpy.array([15, 15, 10, 10])\n",
    "\n",
    "    log_a1, log_a2, log_b0, log_b1 = theta\n",
    "    if ( \n",
    "        bounds_low[0] < log_a1 < bounds_high[0] \n",
    "        and bounds_low[1] < log_a2 < bounds_high[1] \n",
    "        and bounds_low[2] < log_b0 < bounds_high[2] \n",
    "        and bounds_low[3] < log_b1 < bounds_high[3] \n",
    "       ):\n",
    "        return 0.0\n",
    "    return -numpy.inf\n",
    "\n",
    "# We'll use the eztao version which effectively returns \"gp.log_likelihood\" from the GP and np.inf otherwise\n",
    "def lnlike(theta, y, gp):\n",
    "    return -eztao.ts.neg_param_ll(theta, y, gp)\n",
    "\n",
    "def lnprob(theta, y, gp):\n",
    "    lp_bounds = lnprior_bounds(theta)\n",
    "    lp_perturb = lnprior_perturb(theta)                              \n",
    "    if not numpy.isfinite(lp_bounds):\n",
    "        return -numpy.inf\n",
    "    return lp_bounds + lp_perturb + lnlike(theta, y, gp)\n",
    "\n",
    "################################\n",
    "# Define other functions\n",
    "################################\n",
    "\n",
    "# chi-sqared\n",
    "def chisqg(y_data, y_model, sd=None):\n",
    "    chisq = numpy.nansum(((y_data-y_model)/sd)**2)\n",
    "    return chisq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "022f2c88-16b5-42f7-a8bf-78e0b5860e64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getCARMAstats(file):\n",
    "    ################################\n",
    "    # setup\n",
    "    ################################\n",
    "\n",
    "    #file_name = file[22:-8]\n",
    "    file_name = file[22:-4]\n",
    "\n",
    "    \n",
    "    # read-in light curve\n",
    "    df = pandas.read_csv(file)\n",
    "\n",
    "    # obtain values from df\n",
    "    ra = df['ra'].values[0]\n",
    "    dec = df['dec'].values[0]\n",
    "    t = df['mjd'].values\n",
    "    y_real = df['mag'].values\n",
    "    yerr_real = df['magerr'].values\n",
    "    lc_length = len(t)\n",
    "    \n",
    "    # invert the magnitudes\n",
    "    y_real_inverted = (min(y_real)-y_real)\n",
    "\n",
    "    # normalize to unit standard deviation and zero mean\n",
    "    y = (y_real_inverted - numpy.mean(y_real_inverted))/numpy.std(y_real_inverted)\n",
    "    yerr = yerr_real/numpy.std(y_real_inverted)\n",
    "        \n",
    "    \n",
    "    ################################\n",
    "    ################################\n",
    "    #\n",
    "    # DRW Process\n",
    "    #\n",
    "    ################################\n",
    "    ################################\n",
    "    \n",
    "    # obtain best-fit\n",
    "    bounds = [(0.01, 10.0), (0.01, 10.0)]\n",
    "    best_drw = eztao.ts.drw_fit(t, y, yerr, user_bounds=bounds)\n",
    "    \n",
    "    # get best-fit in CARMA space\n",
    "    best_drw_arma = numpy.exp(get_carma_parameter(best_drw[0], best_drw[1]))\n",
    "    \n",
    "    \n",
    "    ################################\n",
    "    ################################\n",
    "    #\n",
    "    # DHO Process\n",
    "    #\n",
    "    ################################\n",
    "    ################################\n",
    "    \n",
    "    # obtain best-fit\n",
    "    bounds = [(-15, 15), (-15, 15), (-20, 10), (-20, 10)]\n",
    "    best_dho = eztao.ts.dho_fit(t, y, yerr, user_bounds=bounds)\n",
    "\n",
    "    # Create the GP model -- instead of creating a \"model\" function that is then called by the \"lnlike\" function from tutorial,\n",
    "    #  we will create a GP that will be passed as an argument to the MCMC sampler. This will be the \"gp\" that is passed to\n",
    "    #  the \"lnprob\" and \"param_ll\" functions\n",
    "    dho_kernel = eztao.carma.DHO_term(*numpy.log(best_dho))\n",
    "    dho_gp = celerite.GP(dho_kernel, mean=numpy.median(y))\n",
    "    dho_gp.compute(t, yerr)\n",
    "\n",
    "    ################################\n",
    "    # MCMC\n",
    "    ################################\n",
    "\n",
    "    # Initalize MCMC\n",
    "    data = (t, y, yerr)\n",
    "    nwalkers = 128\n",
    "    niter = 2048\n",
    "\n",
    "    initial = numpy.array(numpy.log(best_dho))\n",
    "    ndim = len(initial)\n",
    "    p0 = [numpy.array(initial) + 1e-7 * numpy.random.randn(ndim) for i in range(nwalkers)]\n",
    "\n",
    "    # Create the MCMC sampler -- note that the GP is passed as an argument in addition to the data\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, args=[y, dho_gp])\n",
    "\n",
    "    # run a burn-in surrounding the best-fit parameters obtained above\n",
    "    p0, lp, _ = sampler.run_mcmc(p0, 200)\n",
    "    sampler.reset()\n",
    "\n",
    "    # clear up the stored chain from burn-in, rerun the MCMC\n",
    "    pos, prob, state = sampler.run_mcmc(p0, niter);\n",
    "\n",
    "    ################################\n",
    "    # Obtain the Best Fit: theta_max\n",
    "    ################################\n",
    "\n",
    "    # put all the samples that explored in MCMC into a single array\n",
    "    samples = sampler.flatchain\n",
    "    \n",
    "    # find the parameters that have the best fit \n",
    "    theta_max_index = numpy.argmax(sampler.flatlnprobability)\n",
    "    theta_max_probability = sampler.flatlnprobability[theta_max_index]\n",
    "   \n",
    "    theta_max  = samples[theta_max_index] # these are in log-space\n",
    "    theta_max_norm = numpy.exp(theta_max) # take the exponent to get into 'normal' space\n",
    "    \n",
    "    \n",
    "    ################################\n",
    "    ################################\n",
    "    #\n",
    "    # Simulate and Return\n",
    "    #\n",
    "    ################################\n",
    "    ################################\n",
    "    \n",
    "    ################################\n",
    "    # Simulate and plot light curves\n",
    "    ################################\n",
    "    \n",
    "    # create simulated light curve\n",
    "    drw_sim_t, drw_sim_y, drw_sim_yerr = eztao.ts.carma_sim.pred_lc(t, y, yerr, best_drw_arma, 1, t)\n",
    "    dho_sim_t, dho_sim_y, dho_sim_yerr = eztao.ts.carma_sim.pred_lc(t, y, yerr, theta_max_norm, 2, t)\n",
    "    \n",
    "    # directory to save plots to\n",
    "    plot_dir = 'plots_and_figures/carma_plots'\n",
    "    # plot drw\n",
    "    plot = True  \n",
    "    if plot:\n",
    "        matplotlib.pyplot.figure()\n",
    "        matplotlib.pyplot.errorbar(t, y, yerr=yerr, label='data',\n",
    "                                   linestyle=\"None\", marker='.', ms=3., color='purple', ecolor='0.8')\n",
    "        matplotlib.pyplot.plot(drw_sim_t, drw_sim_y, label=f'drw {best_drw_arma}')\n",
    "        matplotlib.pyplot.legend()\n",
    "        matplotlib.pyplot.savefig(f'{plot_dir}/{file_name}_drw_fit.png')\n",
    "        matplotlib.pyplot.close()\n",
    "\n",
    "        # plot dho\n",
    "        matplotlib.pyplot.figure()\n",
    "        matplotlib.pyplot.errorbar(t, y, yerr=yerr, label='data',\n",
    "                                   linestyle=\"None\", marker='.', ms=3., color='purple', ecolor='0.8')\n",
    "        matplotlib.pyplot.plot(dho_sim_t, dho_sim_y, label=f'dho {theta_max_norm}')\n",
    "        matplotlib.pyplot.legend()\n",
    "        matplotlib.pyplot.savefig(f'{plot_dir}/{file_name}_dho_fit.png')\n",
    "        matplotlib.pyplot.close()\n",
    "    \n",
    "    ################################\n",
    "    # Determine best fit\n",
    "    ################################\n",
    "    \n",
    "    # get chi-squared from sim light curves\n",
    "    chisq_drw = chisqg(y, drw_sim_y, yerr)\n",
    "    chisq_dho = chisqg(y, dho_sim_y, yerr)\n",
    "    \n",
    "    # determine best fit\n",
    "    best_fit = 'DRW'\n",
    "    if chisq_drw > chisq_dho and not numpy.isinf(chisq_dho):\n",
    "        best_fit = 'DHO'\n",
    "    \n",
    "    ################################\n",
    "    # Return\n",
    "    ################################\n",
    "    \n",
    "    return file_name, ra, dec, t, y_real, yerr_real, best_drw, best_drw_arma, chisq_drw, best_dho, theta_max_norm, theta_max_probability, chisq_dho, best_fit, lc_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efae6692-c5ae-4f3e-bbba-2c819ec6e3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pp with 24 workers\n",
      "<string>:45: RuntimeWarning: overflow encountered in exp\n",
      " Job execution statistics:\n",
      " job count | % of all jobs | job time sum | time per job | job server\n",
      "         1 |        100.00 |       9.5670 |     9.567021 | local\n",
      "Time elapsed since server creation 9.57351803779602\n",
      "0 active tasks, 24 cores\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ppservers = ()\n",
    "\n",
    "# creates jobserver with ncpus workers\n",
    "ncpus = 24\n",
    "job_server = pp.Server(ncpus, ppservers=ppservers)\n",
    "\n",
    "print(\"Starting pp with\", job_server.get_ncpus(), \"workers\")\n",
    "\n",
    "# get list of data files\n",
    "#repository = glob.glob('data/*.csv')\n",
    "repository = glob.glob('../../AGN_LightCurves/*.parquet')\n",
    "\n",
    "# intialize lists to save to\n",
    "file_names = []\n",
    "times = []\n",
    "magnitudes = []\n",
    "mag_errors = []\n",
    "ras = []\n",
    "decs =[]\n",
    "best_fit_drws = []\n",
    "best_fit_drws_arma = []\n",
    "best_fit_dhos = []\n",
    "best_mcmc_dhos = []\n",
    "dho_probabilities = []\n",
    "chi_squared_drw = []\n",
    "chi_squared_dho = []\n",
    "best_fits = []\n",
    "lc_lengths = []\n",
    "\n",
    "# Submit a list of jobs running getCARMAstats for each file in repository\n",
    "# getCARMAstats - the function\n",
    "# (file,) - file with AGN lc\n",
    "# (chisqg, ...) - tuple with functions on which getCARMAstats depends\n",
    "# (\"numpy\", ...) - tuple with package dependencies to be imported\n",
    "jobs = [(file, job_server.submit(getCARMAstats ,(file,), \n",
    "                                 (get_carma_parameter, lnprior_perturb, lnprior_bounds, lnlike, lnprob, chisqg,), \n",
    "                                 (\"numpy\", \"matplotlib.pyplot\", \"pandas\", \"emcee\", \"eztao\", \"eztao.ts\",\n",
    "                                  \"celerite\"))) for file in repository]\n",
    "\n",
    "job_num = 1\n",
    "for file, job in jobs:\n",
    "    # start job\n",
    "    file_name, ra, dec, t, y, yerr, best_drw, best_drw_arma, chisq_drw, best_dho, best_mcmc_dho, dho_probability, chisq_dho, best_fit, lc_length = job()\n",
    "        \n",
    "    # save data from job\n",
    "    file_names.append(file_name)\n",
    "    ras.append(ra)\n",
    "    decs.append(dec)\n",
    "    times.append(t)\n",
    "    magnitudes.append(y)\n",
    "    mag_errors.append(yerr)\n",
    "    best_fit_drws.append(best_drw)\n",
    "    best_fit_drws_arma.append(best_drw_arma)\n",
    "    chi_squared_drw.append(chisq_drw)\n",
    "    best_fit_dhos.append(best_dho)\n",
    "    best_mcmc_dhos.append(best_mcmc_dho)\n",
    "    dho_probabilities.append(dho_probability)\n",
    "    chi_squared_dho.append(chisq_dho)\n",
    "    best_fits.append(best_fit)\n",
    "    lc_lengths.append(lc_length)\n",
    "    \n",
    "    #print(f'Completed [{job_num}/{len(jobs)}]: {file_name}')\n",
    "    job_num += 1\n",
    "\n",
    "job_server.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc302832-1502-43ed-b962-e01cb756d797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filenames</th>\n",
       "      <th>RA</th>\n",
       "      <th>DEC</th>\n",
       "      <th>Times (MJD)</th>\n",
       "      <th>Magnitudes</th>\n",
       "      <th>Mag Errors</th>\n",
       "      <th>Best DRW Fit</th>\n",
       "      <th>Best DRW ARMA Fit</th>\n",
       "      <th>DRW chisq</th>\n",
       "      <th>Best DHO Fit</th>\n",
       "      <th>DHO MCMC Fit</th>\n",
       "      <th>DHO MCMC Probability</th>\n",
       "      <th>DHO chisq</th>\n",
       "      <th>Best Fit</th>\n",
       "      <th>LC Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>forced_photometry_field600_RA_0.8642750_DEC_27...</td>\n",
       "      <td>0.864275</td>\n",
       "      <td>27.654812</td>\n",
       "      <td>[2458271.9756597, 2458274.9529861, 2458277.946...</td>\n",
       "      <td>[18.106660110135675, 17.987523752735257, 17.96...</td>\n",
       "      <td>[0.1004438952663152, 0.0795561925400699, 0.076...</td>\n",
       "      <td>[1.010050167084168, 22026.465794806718]</td>\n",
       "      <td>[0.3715581744238082, inf]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[3269017.3724721107, 0.009367783625251286, 1.1...</td>\n",
       "      <td>[3269017.3095584773, 0.00936778286760242, 1.16...</td>\n",
       "      <td>-inf</td>\n",
       "      <td>2.50765</td>\n",
       "      <td>DRW</td>\n",
       "      <td>467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Filenames        RA        DEC  \\\n",
       "0  forced_photometry_field600_RA_0.8642750_DEC_27...  0.864275  27.654812   \n",
       "\n",
       "                                         Times (MJD)  \\\n",
       "0  [2458271.9756597, 2458274.9529861, 2458277.946...   \n",
       "\n",
       "                                          Magnitudes  \\\n",
       "0  [18.106660110135675, 17.987523752735257, 17.96...   \n",
       "\n",
       "                                          Mag Errors  \\\n",
       "0  [0.1004438952663152, 0.0795561925400699, 0.076...   \n",
       "\n",
       "                              Best DRW Fit          Best DRW ARMA Fit  \\\n",
       "0  [1.010050167084168, 22026.465794806718]  [0.3715581744238082, inf]   \n",
       "\n",
       "   DRW chisq                                       Best DHO Fit  \\\n",
       "0        0.0  [3269017.3724721107, 0.009367783625251286, 1.1...   \n",
       "\n",
       "                                        DHO MCMC Fit  DHO MCMC Probability  \\\n",
       "0  [3269017.3095584773, 0.00936778286760242, 1.16...                  -inf   \n",
       "\n",
       "   DHO chisq Best Fit  LC Length  \n",
       "0    2.50765      DRW        467  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agn_fit_dat = pandas.DataFrame({'Filenames': file_names, 'RA': ras, 'DEC': decs, 'Times (MJD)': times, \n",
    "                                 'Magnitudes': magnitudes, 'Mag Errors': mag_errors, \n",
    "                                 'Best DRW Fit': best_fit_drws, 'Best DRW ARMA Fit': best_fit_drws_arma, 'DRW chisq': chi_squared_drw,\n",
    "                                 'Best DHO Fit': best_fit_dhos, 'DHO MCMC Fit': best_mcmc_dhos, 'DHO MCMC Probability': dho_probabilities, 'DHO chisq': chi_squared_dho,\n",
    "                                 'Best Fit': best_fits, 'LC Length': lc_lengths})\n",
    "\n",
    "# save dataframe\n",
    "agn_fit_data.to_csv('agn_fit_data.csv')\n",
    "agn_fit_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f66b042-9f0f-4c88-854b-92464adea09b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Combine Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96d714b4-9d66-4a32-96ec-49068a0b56f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affeaca5-0978-4f87-89c8-4f4841a9502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data frame\n",
    "agn_fit_data = pd.read_csv(\"agn_fit_data.csv\")\n",
    "\n",
    "# read in properties data frame\n",
    "agn_properties = pd.read_csv(\"BAT_AGN_BASS_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19977655-e2ce-4fa3-ac01-d34ba72783fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_colemn(df, key, decimals=3):\n",
    "    df[key] = np.around(df[key].tolist(), decimals=decimals)\n",
    "    \n",
    "    # note: return statement is redundant since Python is pass-by-reference\n",
    "    # but return is generally good practice\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eadc6a01-8086-41d7-8f59-a847413ebb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "decimal_places = 1\n",
    "\n",
    "# round fit RA and DEC\n",
    "agn_fit_data = round_colemn(agn_fit_data, 'RA', decimal_places)\n",
    "agn_fit_data = round_colemn(agn_fit_data, 'DEC', decimal_places)\n",
    "\n",
    "# round properties RA and DEC\n",
    "agn_properties = round_colemn(agn_properties, 'RA', decimal_places)\n",
    "agn_properties = round_colemn(agn_properties, 'DEC', decimal_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5187c3e-8421-4aeb-b5b1-f7cc2a74d53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataframes\n",
    "agn_dataframe = pd.merge(agn_fit_data, agn_properties, how='left', on=['RA', 'DEC'], validate='one_to_one')\n",
    "\n",
    "# removed unamed colemns\n",
    "agn_dataframe = agn_dataframe.loc[:, ~agn_dataframe.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# save dataframe\n",
    "agn_dataframe.to_csv('agn_dataframe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2373bd-50d3-40c3-9979-96c5a1986c85",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Bad Fits\n",
    "Exclude fits for DRW if chi-sq = 0 and DHO if chi-sq = inf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4029e89-5a47-4b56-baf6-50333a7b8545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c579a9ac-7f13-4793-9820-9831e0611dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "agn_dataframe = pd.read_csv(\"agn_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cc087ff-dc76-43dc-b7bf-f6036d2c0c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify datarame to account for bad fits\n",
    "# if best chi-sq has a bad fit, then check the other chi-sq\n",
    "# if both fits are bad, then assign chi-sq to 'None'\n",
    "\n",
    "adjusted_best_fits = []\n",
    "\n",
    "for index, row in agn_dataframe.iterrows():\n",
    "    best_fit = row['Best Fit']\n",
    "    drw_chisq = row['DRW_chi_sq']\n",
    "    dho_chisq = row['DHO_chi_sq']\n",
    "    \n",
    "    # if Best Fit is DRW and is bad...\n",
    "    if best_fit == 'DRW' and drw_chisq == 0:\n",
    "        if not np.isinf(dho_chisq):\n",
    "            adjusted_best_fits.append('DHO')\n",
    "        else:\n",
    "            adjusted_best_fits.append('None')\n",
    "            \n",
    "    # if Best Fit is DHO and is bad...\n",
    "    elif best_fit == 'DRW' and np.isinf(dho_chisq):\n",
    "        if not drw_chisq == 0:\n",
    "            adjusted_best_fits.append('DRW')\n",
    "        else:\n",
    "            adjusted_best_fits.append('None')\n",
    "    \n",
    "    # if Best Fit is good, then save it\n",
    "    else:\n",
    "        adjusted_best_fits.append(best_fit)\n",
    "        \n",
    "# replace adjusted values\n",
    "#agn_dataframe['Best Fit'] = agn_dataframe['Best Fit'].replace(adjusted_best_fits)\n",
    "agn_dataframe['Best Fit'] = adjusted_best_fits\n",
    "\n",
    "# removed unamed colemns\n",
    "agn_dataframe = agn_dataframe.loc[:, ~agn_dataframe.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# save dataframe\n",
    "agn_dataframe.to_csv('adjusted_agn_dataframe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ad366a-60c9-4fb3-98f2-b38a2132b9d1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Calculate SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fd19ddb-51d8-4590-af18-73316898da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "468450a4-edaa-4a02-a9eb-b5f6b708e75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agn_dataframe = pd.read_csv(\"adjusted_agn_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "036fd557-d11b-48f5-911b-c9eaa190cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate snr for each row in the data frame\n",
    "snr = []\n",
    "\n",
    "# index each AGN in agn_fit_data and calculate SNR\n",
    "for index, row in agn_dataframe.iterrows():\n",
    "    # get data from row\n",
    "    y = row['Magnitudes']\n",
    "    err = row['Mag Errors']\n",
    "    \n",
    "    # convert into float lists\n",
    "    y = convert_to_float_list(y)\n",
    "    err = convert_to_float_list(err)\n",
    "    \n",
    "    # mean of mag and err\n",
    "    if y != flag:\n",
    "        mean_y = np.mean(y)\n",
    "        mean_err = np.mean(err)\n",
    "        snr_row = mean_y / mean_err\n",
    "    else:\n",
    "        snr_row = np.NAN\n",
    "    \n",
    "    # save data\n",
    "    snr.append(snr_row)\n",
    "\n",
    "# add snr\n",
    "agn_dataframe['SNR'] = snr\n",
    "\n",
    "# save to new .csv file\n",
    "agn_dataframe.to_csv('adjusted_agn_dataframe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4413dd3-35b2-4436-a4ef-a51e76b5affb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove any unamed colemns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5a1e057-4a4e-42e9-a4fd-f811e3859213",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'adjusted_agn_dataframe.csv'\n",
    "agn_dataframe = pd.read_csv(filename)\n",
    "\n",
    "# removed unamed colemns\n",
    "agn_dataframe = agn_dataframe.loc[:, ~agn_dataframe.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# save dataframe\n",
    "agn_dataframe.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1672fb36-dbab-4b94-bf2f-943fd4484c87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Timescales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b22947d6-f4fb-44e1-91e1-590ec5eb124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11d0281c-018a-4382-9ef3-a2a98198d37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agn_dataframe = pd.read_csv('adjusted_agn_dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8e0773c-d8de-4062-a8fd-50b813eeeda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dho_timescales(params):\n",
    "    \"\"\"Compute a couple DHO timescales from CARMA parameters (in normal space).\n",
    "\n",
    "    - damping factor\n",
    "    - decay timescale\n",
    "    - rise/damped QPO timescale\n",
    "    - perturbation timescale\n",
    "    - decorrelation timescale\n",
    "    - natural oscillation frequency\n",
    "    \"\"\"\n",
    "   \n",
    "    # expand params\n",
    "    a1, a2, b0, b1 = params  \n",
    "\n",
    "    # damping factor & natural frequency\n",
    "    xi = a1/(2*np.sqrt(a2))\n",
    "    omega_0 = np.sqrt(a2)   \n",
    "\n",
    "    # placeholder for two timescales\n",
    "    tau_perturb = b1/b0\n",
    "    tau_decay = 0\n",
    "    tau_rise_dqpo = 0\n",
    "    tau_decorr = 0\n",
    "\n",
    "    roots = np.roots([1, a1, a2])\n",
    "    if xi < 1:\n",
    "        tau_decay = np.abs(1/roots[0].real)\n",
    "        tau_rise_dqpo = 2*np.pi*np.abs(1/roots[0].imag)/np.sqrt(1 - xi**2)\n",
    "        tau_decorr = (np.pi/2)*np.pi*2/omega_0\n",
    "    else:\n",
    "        tau_decay = np.abs(1/np.max(roots.real))\n",
    "        tau_rise_dqpo = np.abs(1/np.min(roots.real))\n",
    "        tau_decorr = (tau_decay + tau_rise_dqpo)*np.pi/2\n",
    " \n",
    "    return np.array([xi, tau_decay, tau_rise_dqpo, tau_perturb, tau_decorr, omega_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a9376db-d796-49eb-80f4-e615d919011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = [float('inf')]\n",
    "\n",
    "# Converts a string into a float list.\n",
    "# String must be in the following format:\n",
    "# '[1.01, 2.02, 3.03, 4.04]'\n",
    "# Returns an array of len=1 with the one index='inf' if the String\n",
    "# cannont be converted into a list of floats.\n",
    "# Takes an optional string char for the characters to split by\n",
    "def convert_to_float_list(str_list, **kwargs):\n",
    "    char = kwargs.get('char', None)\n",
    "    debug = kwargs.get('debug', False)\n",
    "    \n",
    "    if debug:\n",
    "        print(str_list + ' foo')\n",
    "        \n",
    "    str_list = str_list.replace('[', '')\n",
    "    str_list = str_list.replace(']', '')\n",
    "    \n",
    "    if debug:\n",
    "        print(str_list + ' bar')\n",
    "    \n",
    "    try:\n",
    "        return [float(i) for i in str_list.split(char)]\n",
    "    except ValueError:\n",
    "        return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54c1d78b-4927-462b-9581-151eeba080c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bat_ids = []\n",
    "xis = []\n",
    "tau_decays = []\n",
    "tau_rise_dqpos = []\n",
    "tau_perturbs = []\n",
    "tau_decorrs = []\n",
    "omega_0s = []\n",
    "\n",
    "for index, row in agn_dataframe.iterrows():\n",
    "    bat_id = row['BAT ID']\n",
    "    dho_params = convert_to_float_list(row['DHO MCMC Fit'])\n",
    "    timescales = dho_timescales(dho_params)\n",
    "    \n",
    "    bat_ids.append(bat_id)\n",
    "    xis.append(timescales[0])\n",
    "    tau_decays.append(timescales[1])\n",
    "    tau_rise_dqpos.append(timescales[2])\n",
    "    tau_perturbs.append(timescales[3])\n",
    "    tau_decorrs.append(timescales[4])\n",
    "    omega_0s.append(timescales[5])\n",
    "    \n",
    "timescales_df = pd.DataFrame({'xi': xis, \n",
    "                              'tau_decay': tau_decays, 'tau_rise_dqpo': tau_rise_dqpos, 'tau_perturb': tau_perturbs, 'tau_decorr': tau_decorrs, \n",
    "                              'omega_0': omega_0s})\n",
    "\n",
    "agn_dataframe.join(timescales_df).to_csv('adjusted_agn_dataframe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f9b587-7ba2-4f29-b02c-a7823559458a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Overdamped and QPO's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f34b03bb-9038-44df-8b8c-855771083d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfdaba96-a7f2-4dd5-8efa-f65b27d1c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data frame\n",
    "agn_dataframe = pd.read_csv('agn_dataframe.csv')\n",
    "\n",
    "# determined if overdamped or qpo\n",
    "xis = agn_dataframe['xi']\n",
    "oscillations = []\n",
    "\n",
    "for xi in xis:\n",
    "    if xi > 1:\n",
    "        oscillations.append('overdamped')\n",
    "    elif xi < 1:\n",
    "        oscillations.append('underdamped')\n",
    "    else:\n",
    "        oscillations.append('critically damped')\n",
    "        \n",
    "agn_dataframe['oscillation'] = oscillations\n",
    "\n",
    "# save dataframe\n",
    "agn_dataframe.to_csv('agn_dataframe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51caac9c-d43d-4863-960f-ae7483936ce3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Class and Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04332632-2540-431d-9994-e0dc7cbb01dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7119c5-bda9-4fcc-890e-9dbc4f4ecaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pd.read_csv('BAT_70m_catalog_20nov2012.txt', sep='|')\n",
    "\n",
    "# removed unamed colemns\n",
    "catalog = catalog.loc[:, ~catalog.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# round fit RA and DEC\n",
    "decimal_places = 1\n",
    "catalog = round_colemn(catalog, 'CTPT_RA', decimal_places)\n",
    "catalog = round_colemn(catalog, 'CTPT_DEC', decimal_places)\n",
    "\n",
    "# create new dataframe with RAs, Decs, and desired data\n",
    "catalog = pd.DataFrame({'RA': catalog['CTPT_RA'], 'DEC': catalog['CTPT_DEC'],\n",
    "                       'CL2': catalog['CL2'], 'TYPE': catalog['TYPE']})\n",
    "catalog.to_csv('catalog.csv')\n",
    "catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c36a34e-e29a-47f0-a48d-3b6c0f5965b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data frame\n",
    "agn_dataframe = pd.read_csv(\"agn_dataframe.csv\")\n",
    "\n",
    "# merge dataframes\n",
    "agn_dataframe = pd.merge(agn_dataframe, catalog, how='left', on=['RA', 'DEC'], validate='one_to_one')\n",
    "\n",
    "# removed unamed colemns\n",
    "agn_dataframe = agn_dataframe.loc[:, ~agn_dataframe.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# save dataframe\n",
    "agn_dataframe.to_csv('agn_dataframe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf6852a-a37d-4ed5-bf62-5904ebacb818",
   "metadata": {
    "tags": []
   },
   "source": [
    "### tau_rise vs. omega_0 line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d581c66-8e2f-475a-a6f0-80322f2dfef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d58a9eba-56a2-4d85-96ba-cac101d67488",
   "metadata": {},
   "outputs": [],
   "source": [
    "agn_dataframe = pd.read_csv('agn_dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f95bd7b-4d74-4f1c-ae88-178d4ae66b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute power function through two arbitrary points.\n",
    "# Where the function is y = y1 / (x1^p) * x^p\n",
    "p1 = [10e-4, 2000]\n",
    "p2 = [10e3, 2e-4]\n",
    "\n",
    "# the slope/power\n",
    "# p = (np.log(p2[1]) - np.log(p1[1])) / (np.log(p2[0]) - np.log(p1[0]))\n",
    "p = np.log(p2[1]/p1[1]) / np.log(p2[0]/p1[0])\n",
    "\n",
    "\n",
    "# if tau_rise > y for each x = omega_0, then flag \n",
    "##### HOWEVER\n",
    "# note that in the data, some points exist above the trendline AND outside the overall trend\n",
    "flag = []\n",
    "for index, row in agn_dataframe.iterrows():\n",
    "    x = row['omega_0']\n",
    "    y = row['tau_rise_dqpo']\n",
    "    \n",
    "    trendline = (p1[1]/(p1[0]**p)) * (x**p)\n",
    "    on_trendline = y > trendline\n",
    "    \n",
    "    flag.append(on_trendline)\n",
    "    \n",
    "agn_dataframe['rise-omega trendline'] = flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76208560-ea7f-4d5e-8f31-87831ebd0747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed unamed colemns\n",
    "agn_dataframe = agn_dataframe.loc[:, ~agn_dataframe.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# save dataframe\n",
    "agn_dataframe.to_csv('agn_dataframe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5360f367-d9eb-447a-a923-2480bf827945",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### \"Bad Fits\" based on timescales\n",
    "- timescales that are smaller than half the minimum distance between two points in the light curve (e.g., min(delta_t)) or larger than twice the length of the light curve.\n",
    "- timescales to reference: tau_decorr, tau_decay, and tau_rise\n",
    "\n",
    "Possible limits:\n",
    "- tau_decay > 10<sup>3</sup>\n",
    "- tau_decorr < 10<sup>-2</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e64e60d-1bbe-449d-a92b-6f9c0bc9ab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d8e46a6-181b-4334-be88-7606b8fbf459",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = [float('inf')]\n",
    "\n",
    "# Converts a string into a float list.\n",
    "# String must be in the following format:\n",
    "# '[1.01, 2.02, 3.03, 4.04]'\n",
    "# Returns an array of len=1 with the one index='inf' if the String\n",
    "# cannont be converted into a list of floats.\n",
    "# Takes an optional string char for the characters to split by\n",
    "def convert_to_float_list(str_list, **kwargs):\n",
    "    char = kwargs.get('char', None)\n",
    "    debug = kwargs.get('debug', False)\n",
    "    \n",
    "    if debug:\n",
    "        print(str_list + ' foo')\n",
    "        \n",
    "    str_list = str_list.replace('[', '')\n",
    "    str_list = str_list.replace(']', '')\n",
    "    \n",
    "    if debug:\n",
    "        print(str_list + ' bar')\n",
    "    \n",
    "    try:\n",
    "        return [float(i) for i in str_list.split(char)]\n",
    "    except ValueError:\n",
    "        return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3030e1f4-9583-4078-a634-b10f90b2f0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Caleb\\AppData\\Local\\Temp/ipykernel_12336/1308568785.py:46: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  min_interval = (curr_interval, min_interval)[min_interval < curr_interval]\n"
     ]
    }
   ],
   "source": [
    "agn_dataframe = pd.read_csv('agn_dataframe.csv') \n",
    "\n",
    "################\n",
    "# Create empty lists to save values to\n",
    "################\n",
    "\n",
    "good_timescale_fits = []\n",
    "\n",
    "decorr_fits = []\n",
    "decay_fits = []\n",
    "rise_fits = []\n",
    "\n",
    "################\n",
    "# Iterate light curves\n",
    "################\n",
    "\n",
    "# for each light curve\n",
    "for index, row in agn_dataframe.iterrows():\n",
    "    ################ \n",
    "    # Read in time\n",
    "    ################\n",
    "    t = convert_to_float_list(row['Times (MJD)'])\n",
    "    \n",
    "    if np.isinf(t[0]):\n",
    "        # then grab t from raw data\n",
    "        filename = row['Filenames']\n",
    "        \n",
    "        # read file\n",
    "        file_dir = '../../AGN_LightCurves/'\n",
    "        file_type = '.parquet'\n",
    "        df = pd.read_csv(file_dir + filename + file_type)\n",
    "\n",
    "        # grab time\n",
    "        t = df['mjd'].values\n",
    "        \n",
    "    ################ \n",
    "    # Get half of the min time interval\n",
    "    ################\n",
    "    \n",
    "    # initialize min_t as the 32 bit integer limit\n",
    "    min_interval = np.iinfo(np.int32).max\n",
    "   \n",
    "    # find min time interval\n",
    "    for i in range(len(t)-2):\n",
    "        curr_interval = t[i+1] - t[i]\n",
    "        min_interval = (curr_interval, min_interval)[min_interval < curr_interval]\n",
    "        \n",
    "    # save 1/2 * min(delta_t)\n",
    "    min_t = 0.5 * min_interval\n",
    "    \n",
    "    ################ \n",
    "    # Get twice the timeframe\n",
    "    ################\n",
    "    \n",
    "    timeframe = t[len(t)-1] - t[0]\n",
    "    max_t = timeframe * 2 \n",
    "    \n",
    "    ################\n",
    "    # Compare time with timescales\n",
    "    ################\n",
    "    \n",
    "    tau_decay = row['tau_decay']\n",
    "    tau_rise = row['tau_rise_dqpo']\n",
    "    tau_decorr = row['tau_decorr']\n",
    "    \n",
    "    # Compare and assign value based on result\n",
    "        # ontime = good\n",
    "        # undertime = < min\n",
    "        # overtime = > max\n",
    "    def timescale_fit(timescale, max_t, min_t):\n",
    "        if timescale > max_t:\n",
    "            fit = 'overtime'\n",
    "        elif timescale < min_t: \n",
    "            fit= 'undertime'\n",
    "        else:\n",
    "            fit = 'ontime'\n",
    "        return fit\n",
    "    \n",
    "    decay_fit = timescale_fit(tau_decay, max_t, min_t)\n",
    "    rise_fit = timescale_fit(tau_rise, max_t, min_t)\n",
    "    decorr_fit = timescale_fit(tau_decorr, max_t, min_t)\n",
    "    \n",
    "    # Save as single boolean\n",
    "        # TRUE = good fit\n",
    "        # FALSE = bad fit\n",
    "    good_timescale_fit = 'ontime' == decay_fit == rise_fit == decorr_fit\n",
    "        \n",
    "    # append to lists\n",
    "    good_timescale_fits.append(good_timescale_fit)\n",
    "    decay_fits.append(decay_fit)\n",
    "    rise_fits.append(rise_fit)\n",
    "    decorr_fits.append(decorr_fit)\n",
    "\n",
    "################\n",
    "# Update datframe\n",
    "################\n",
    "\n",
    "# add to dataframe in four colmens\n",
    "agn_dataframe['good timescale fit'] = good_timescale_fits\n",
    "agn_dataframe['tau_decay fit'] = decay_fits\n",
    "agn_dataframe['tau_rise_dqpo fit'] = rise_fits\n",
    "agn_dataframe['tau_decorr fit'] = decorr_fits\n",
    "\n",
    "# removed unamed colemns\n",
    "agn_dataframe = agn_dataframe.loc[:, ~agn_dataframe.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# save dataframe\n",
    "agn_dataframe.to_csv('agn_dataframe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4697b80a-221e-45fa-8954-afb1a9ee6f46",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Fix missing data (t, mag, mag_err)\n",
    "~~For some reason, some LC were saved a [# # # ... # # #] where all the data in between is missing. This section aims to reinput all of the missing data.~~\n",
    "There appears to be a limitation with pandas where the max np.array length saved in a cell is ~1000 indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8d9ce2c-f225-4812-98bc-0cb0e12d15ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b264670c-46fa-4cd7-820a-867db158a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note bad indeces as hardcoded list\n",
    "# (indeces found when looking for bad fits with timescales)\n",
    "bad_indeces = [134, 174, 205, 208, 223, 224, 228, 231, 237, 238, 244, 249, 250]\n",
    "\n",
    "# go through agn_dataframe and save file names for all the bad indeces\n",
    "agn_dataframe = pd.read_csv('agn_dataframe.csv') \n",
    "filenames = agn_dataframe.iloc[bad_indeces]['Filenames']\n",
    "\n",
    "# for each index\n",
    "for index, filename in filenames.iteritems():    \n",
    "    # read file\n",
    "    file_dir = '../../AGN_LightCurves/'\n",
    "    file_type = '.parquet'\n",
    "    df = pd.read_csv(file_dir + filename + file_type)\n",
    "    \n",
    "    # create a list for each data type in file\n",
    "    t = df['mjd'].values\n",
    "    mag = df['mag'].values\n",
    "    mag_err = df['magerr'].values\n",
    "    \n",
    "    # Update each list for respctive index in agn_dataframe\n",
    "    # (i.e. agn_datame[134]['Time(Mjd)'] = times\n",
    "    agn_dataframe.at[index, 'Times (MJD)'] = t\n",
    "    agn_dataframe.at[index, 'Magnitudes'] = mag\n",
    "    agn_dataframe.at[index, 'Mag Errors'] = mag_err\n",
    "    \n",
    "# removed unamed colemns\n",
    "agn_dataframe = agn_dataframe.loc[:, ~agn_dataframe.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# save dataframe\n",
    "agn_dataframe.to_csv('agn_dataframe_tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4439a0de-7b1e-4cf6-a963-f2c1c68a0d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_ind = np.where((row['mag'] > 10) & (row['mag'] < 30))[0]\n",
    "\n",
    "t = row['JD'][good_ind]\n",
    "y = row['mag'][good_ind]\n",
    "e = row['magerr'][good_ind]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
